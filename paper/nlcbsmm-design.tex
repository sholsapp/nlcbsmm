\section{System Design}
\label{system-design}

The \projname{} system presented in this paper is a software distributed shared memory systems.  Like Treadmarks in \cite{Amza:1996:TSM:226705.226708,Keleher:1994:TDS:1267074.1267084}, the \projname{} system is implemented entirely in user-space, making complicated kernel-based modifications unnecessary.  The \projname{} system features a centralized manager and is designed to use the respective servers presented by Li and Hudak in \cite{Li:1989:MCS:75104.75105}.  The \projname{} system maintains an unstructured memory region.  In fact, it makes no modifications to the memory structure used by the underlying application unless otherwise noted.  This allows the \projname{} system to be used by any application without the need for source code modification, as is the case in some type-specific coherence systems that may require annotations to label memory.  Research has shown that lazy release consistency is an ideal memory coherence model for SDSM systems.  As such, the \projname{} system will be designed to exhibit a lazy release memory coherence model.  For comparison, the \projname{} system will be able to exhibit other memory coherence models, too, depending on what application synchronization primitives are used.

This chapter discusses the assumptions, design considerations, and design decisions of the \projname{} system.  The implementation of the design presented in this chapter is the subject of future chapters.

\subsection{Assumptions}
\label{assumptions}

There are several assumptions that were necessary to make for implementation reasons.  The following is a discussion of each of those assumptions, as well as a brief discussion regarding why the assumption was necessary.

\begin{description}
\item[Operating System] \hfill \\
The \projname{} system requires that the distributed system be run on identical operating systems.  This is partly due to the system's reliance on the application binary (e.g., exported symbols) and program loader (e.g., placement of loadable segments).  This assumption assumes that the same compilation toolchain is used on each member of the cluser.

\item[Threading Library Requirements] \hfill \\
 The \projname{} system is designed to distribute work across multiple execution cores.  The only feasible way to do this is to require that the target application be written using a threading library.  The \projname{} system requires that the target application use the \verb,pthread, library, and also assumes that the target application is properly written using appropriate \verb,pthread, provided synchronization primitives like \verb,pthread_mutex,.  Currently, the \projname{} system only supports \verb,pthread_mutex,.

\item[Compilation Requirements] \hfill \\
The \projname{} system is a shared library.  As such, the target application must be linked with and loaded with the \projname{} library.  This allows \projname{} to replace function definitions for important functions like \verb,pthread_create, and \verb,malloc,.
\end{description}


\subsection{Virtual Memory Management Design}
\label{vmm-design}

Typical virtual memory management (VMM) systems operate in kernel-space, hiding virtually all aspects of memory management to the application programmer.  For many types of applications this is permissible, and even preferred.  However, for a SDSM system built in user-space, the transparent management of memory is a problem that must be addressed by emulating its behavior in user-space.

Running a VMM in user-space is non-trivial and requires understanding and control of many system-specific phenomena.  The understanding and control required is compounded when more than one process running separate address spaces are expected to use the same VMM.  These challenges and system-specific phenomena are the topic of this section.

\subsubsection{Address Space Utilization}
Virtual memory fools application programs to believe the program has access to a contiguous range of memory.  The system provided VMM does this by mapping process identification and virtual addresses to available physical addresses located in physical memory via lookup tables like the \em page table\em.  This is done transparently.  That is, an application never sees or requires the address of a physical page of memory.  Rather, these tasks are that of the underlying kernel.  For applications that wish to manage memory themselves or for other applications, this is problematic because the underlying placement of memory is invisible to the application.  This makes synchronizing memory between multiple processes challenging.

The strategy taken the \projname{} system is to leave the system provided VMM in place and add an additional user-space VMM to the application program.  This has the advantage of allowing one to record the placement of virtual pages, while leaving the task of mapping the virtual page into the underlying system's physical memory.  The truth is, for a SDSM system one does not care what physical memory is being used.

Using this design, the \projname{} system can broadcast what memory it has available in each of the nodes' virtual memory without the need to know about each nodes' underlying physical memory.  It is the task of the user-space VMM to then synchronize available memory and memory permissions to participating nodes.

\subsubsection{Address Space Synchronization}

We have assumed that the \projname{} system is compiled as a shared library, and that the target application is linked and loaded with this shared library.  As a result, the library code exists in the same address space as the target application during the lifetime of the application.  This has several non-obvious implications that make building a distributed VMM difficult.  

A naive VMM design places \projname{} heap allocated memory in the target application's heap area.  This has the effect of mixing \projname{} heap allocated memory with the target application's heap allocated memory.  The problem of deciphering what memory is belongs to \projname{} versus what memory is belongs to the target application is difficult.  This problem is compounded when one considers that \projname{}'s and the target application's heap allocated memory are mixed on a single memory unit.  Additionally, heap addresses tend to differ between processes which necessitate tedious memory remapping.  This makes synchronization virtually impossible.

Therefore, the first challenge is that of enforcing a clean separation of \projname{} heap allocated memory from the target application's heap allocated memory.  By enforcing a clean separation of \projname{} from the target application, the problem of synchronization is far less difficult.  Using this design, \projname{} may maintain a VMM in a separate region of memory that can be synchronized with other \projname{} instances in the cluster.  Additionally, this allows us to fix critical \projname{} memory addresses in memory.  Because we have assumed we're running the same target application on the same underlying architecture, we can use fixed addresses and offsets to choose a safe area for \projname{}'s heap.  Lastly, because an entire region of memory is being synchronized between projects, objects like those in the Standard Template Library can be used between processes.  This makes building complex data structures simple.

\subsubsection{Fault Resolution}

A distributed node, having been alerted to the shared memory mappings of the network cluster, needs a mechanism to access the memory that may not be mapped into its own virtual memory.  Functionally, this allows a distributed node to read and write memory that another distributed node has mapped into its own virtual memory address space.

To achieve this, Linux signal handling tools may be employed to detect and resolve memory access attempts.  For example, one may detect memory accesses by using the \verb,mprotect, system call to set access permissions on memory pages.  Furthermore, illegal accesses to these memory pages will result in the generation of a \verb,SIGSEGV, signal.  By designing a signal handler for \verb,SIGSEGV, one may enforce permissions on memory.  Additionally, one may even resolve the illegal memory access by remapping memory and allowing the offending instruction to \em try \em access memory again.  Ideally, after remapping memory, the offending instruction will execute without illegally accessing memory.  This strategy is identical to the strategy used by Treadmarks in \cite{Keleher:1994:TDS:1267074.1267084}.

\subsection{Network Cluster Coordinator}
\label{network-stack-design}

\subsubsection{Bootstrapping}
Bootstrapping a distributed shared memory system is a non-trivial task that requires careful handling of several items closely related to the underlying system.  The most important tasks include the loading of the target-application and verification and comparison of address spaces features.  Loading an application designed to run on a single machine is a trivial problem where most system-specific intricacies are hidden from the application programmer by their system-provided toolchain.  However, loading an application designed to run on a single machine on a cluster of machines is non-trivial.

Because of the assumptions that the \projname{} system maintains in regards to compilation requirements and operating system similiarity, we designed \projname{} in a fashion that allows us to run a \em single \em application binary on each networked machine.  This \em single \em application binary contains all code to operate as both a master and a worker.  This application binary is compiled, linked and loaded with the same compiler toolchain and and loaded on identical architectures.  This has several beneficial side-effects.  Firstly, a single application is distributed between the nodes.  Secondly, the object file on disk is identical.  Because the same toolchain and operating systems is required, the loaded process image guarantees address space uniformity: loadable segments share the same addresses.  The only items that aren't guaranteed to be identical is the dynamic linker's placement choices for shared libraries.  However, in practice we've seen that they are similiar enough.  We use heuristics to identify where process-specific shared libraries exist and take measures to ensure that each process is aware of the placement of other processes' shared libraries.

Another non-obvious challenge that the bootstrapping design must address is a facility to hault the target-application until a message or signal is passed to it.  Applications designed to run on a single machine need not worry about this design issue because once an application is loaded it is ready to run code -- presumably its \verb,main, entrypoint.  This is not the case in a distributed system.  The \projname{} bootstrapping system is tasked with determining a master node, waiting for worker nodes to join and initialize themselves, and lastly, signalling a \em single \em target-application to call its entry point.

\subsubsection{Cluster Coordination}
There are several networking facilities that the \projname{} system must be able to support.  These facilities are a combination of multicast and unicast communication styles.  However, the ability to communicate with other nodes is a trivial problem.  Instead, the task of the cluster coordinator is really to detect \em when \em certain networking events should happen.  For example, \em when \em should a master node synchronize its data with a new worker or \em when \em should a master create or join threads on a worker node are events that the cluster coordinator must identify.

Because of the assumption that the target-application is a \verb,pthread, based program linked with the \projname{} library, we can make use of several system tricks to help identify interesting events.  First, the \projname{} library can make use of symbol manipulation to replace the implementations of functions like \verb,pthread_create, or \verb,pthread_join, with different implementations with stronger linkage.  Second, the \projname{} system can make use of allocator hooks to instrument or completely replace the implementation of \verb,malloc, and \verb,free,.  Combining these strategies provides a solid foundation to populate an event queue capable of powering a SDSM system.

The \projname{} system requires an event-driven networking infrastructure to communicate the events previously described.  We found that using a mixture of multicast and unicast datagrams made the implementation most straight-forward.  Furthermore, we found that using a single client-server combination was inappropriate, and opted to maintain multiple servers per-process.  Among other reasons, using this design the \projname{} system is capable of performing blocking work without halting the servers responsible for listening and queueing important events in the cluster.


\subsubsection{Coherence Semantics}
One of the most important features the \projname{} system must support is a weak memory coherency model.  Though a DSM system may operate correctly using a stricter coherency model, efficiency and communication overhead become serious concerns.  Supporting a weak coherency model maximizes efficiency by minimizing the communication overhead involved in maintaining a global address space.

To implement such a coherency model, the \projname{} system needs to know certain information about the access patterns the target-application exhibits.  In systems like Munin~\cite{Keleher:1992:LRC:139669.139676, Bennett:1990:MDS:99163.99182, Carter:1991:IPM:121132.121159, Zwaenepoel:1992:MDS:134397.135235}, this is done through memory annotations.  This model was considered too restrictive for \projname{}, as it requires source code annotations or an intelligent runtime system to categorize memory accesses.  Because of the assumption that the target-application is a \verb,pthread, based program, we can exploit synchronization primitives like \verb,pthread_mutex,s to enforce a weaker consistency model.  

A weak consistency model can be designed in \projname{} by intercepting \verb,pthread_mutex_lock, and \verb,pthread_mutex_unlock,.  When a worker node acquires a lock and receives confirmation that the lock was acquired from the central manager, that worker can record what pages are dirtied during its execution.  Upon releasing the lock, that worker can transmit what pages it has invalidated to the central manager.  The central manager can then lazily propagate these invalidated pages to future acquirers of the same mutex.  This approach hangs program correctness on the locking used in the application.

This approach requires no \em additional \em source code annotations.  We feel that assuming correctly written applications is a reasonable assumption.  There are several interesting caveats that must be taken into consideration when determining what types of applications are correctly written.  For example, most \verb,pthread, based applications can make the assumption that the threads are running within the same address space on the same machine.  This assumption often leads to a program that allows threads to read or write memory concurrently without the use of locks or barriers, as long as those threads are not reading and writing the \em same \em memory concurrently.  One practical example of this type of application is a \verb,pthread,-enabled version of matrix multiply.  Running this type of application on a DSM will be inefficient because of the potential for the threads to thrash over the ownership of a memory page.  Applications that do not make this assumption and use locks and barriers to synchronize read and write accesses to memory will realize the full power of the weaker consistency models.




